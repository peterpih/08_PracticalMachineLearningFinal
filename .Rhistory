head(q)
train(y~x, data=q,method="lm")
fit3 <-train(y~x, data=q,method="lm")
summary(fit3)
dat
fit
fit4 <-train(y~x, data=q,method="glm")
summary(fit4)
fit
summary(fit)
fit4 <-train(y~x, data=q,method="glm")
predict(fit)
y
fit4
summary(fit4)
summary(fit)
summary(fit3)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)
q<-cbind(x=x)
q<- cbind(q, y=y)
q
fit_train <= train(x~., data=q,method="glm")
fit_train <- train(x~., data=q,method="glm")
library(caret)
fit_train <- train(x~., data=q,method="glm")
fit_train <- train(x~., data=q,method="lm")
fit_train <- train(y~., data=q,method="glm")
fit_train <- train(y~x, data=q,method="glm")
fit_train <- train(q$y~., data=q,method="glm")
rm(q)
q<- cbind(x1=x)
q<-cbind(q,y1=y)
fit_t <- train(y1~.,data=q,method="glm")
fit_t
fit
summary(fit_t)
?train
train
str(q)
x
wgt = rep(1, 9)
fit_t <- train(y1~.,data=q,weights=w, method="glm")
fit_t <- train(y1~.,data=q,weights=wts, method="glm")
fit_t <- train(y1~.,data=q,weights=wgt, method="glm")
?predict
predict(fit, x)
predict.glm(fit_t, newdata=x)
plot(x,y)
abline(fit)
class(fit)
abline(fit, color="red")
abline(fit, colour="red")
abline(fit, col="red")
plot(x,y)
abline(fit, col="red")
mtcars
y <- mtcars$mpg
x <- mtcars$wt
fit<-lm(x,y)
fit<-lm(y~x)
predict(fit, x, interval="confidence")
x
dataframe(x)
data.frame(x)
predict(fit, data.frame(x), interval="confidence")
predict(fit, data.frame(mean(x), interval="confidence")
predict(fit, data.frame(mean(x)), interval="confidence")
predict(fit, data.frame(x=mean(x)), interval="confidence")
predict(fit, data.frame(mean(x)), interval="confidence")
mean(x)
w<-mean(x)
w
predict(fit, data.frame(w), interval="confidence")
d<-rep(w,32)
d
predict(fit, data.frame(d), interval="confidence")
predict(fit, data.frame(x=d), interval="confidence")
predict(fit, data.frame(d), interval="confidence")
d
data.frame(x=mean(x))
data.frame(y=mean(x))
predict(fit, data.frame(x=mean(x)), interval="confidence")
predict(fit, data.frame(y=mean(x)), interval="confidence")
str(fit)
x2 <- x
fit2 <- lm(y~x2)
str(fit2)
predict(fit, data.frame(x=mean(x)), interval="confidence")
predict(fit, data.frame(x2=mean(x)), interval="confidence")
fit
predict(fit, data.frame(x=mean(x)), interval="confidence")
predict(fit, data.frame(x=mean(x)), interval="prediction")
predict(fit, data.frame(x=3000), interval="prediction")
predict(fit, data.frame(x=3000), interval="confidence")
predict(fit, data.frame(x=3), interval="confidence")
predict(fit, newdata=data.frame(3))
predict(fit, newdata=data.frame(3), interval="confidence")
predict(fit, newdata=data.frame(x =3), interval="confidence")
x
I(x/2)
x/2
summary(fit)
summary(fit)$coefficients
summary(fit)$residuals
min(summary(fit)$residuals)
max(summary(fit)$residuals)
str(fit)
fit$df.residuals
fit$df.residual
fit$df.coefficients
fit$coefficients
fit$x
fit$rank
fit$model
fit$model$x
fit$model$intercept
fit$model.intercept
predict(fit, data.frame(x=3),method="confidence")
predict(fit, data.frame(x=3),method="interval")
?predict
predict.lm(fit, data.frame(x=3),method="interval")
predict.lm(fit, data.frame(x=3),interval="confidence")
predict.lm(fit, data.frame(x=3),interval="prediction")
predict.lm(fit, data.frame(x=2),interval="prediction")
y<= mtcars$mpg
y<- mtcars$mpg
y
x<- mtcars$wt
fit<-lm(y~x)
predict(fit, newdata=data.frame(x=2), interval="prediction")
rm(list=ls())
x <- rnorm(20)
y<-rnorm(20)
z <- x+y
fit <- lm(y~x+z)
fit
lm(y~x, data=z)
library(datasets)
data(swiss)
require(stats)
require(graphics)
str(swiss)
swiss
x <- rnorm(20)
y<- rnorm(20)
plot(x,y)
fit <-lm(y~x)
predict(fit)
y
abline(fit)
y_hat <-predict(fit)
plot(x,y_hat)
abline(fit)
predict(fit, newdata=data.frame(x))
x2 <-rnorm(20)
predict(fit, newdata=data.frame(x2))
predict(fit, data=data.frame(x2))
?predict
predict(fit, data=data.frame(x=x2))
x2
x1
x
fit$coefficients
x2 * fit$coefficients[2] + fit$coefficients[1]
y3<-x2 * fit$coefficients[2] + fit$coefficients[1]
plot(x2,y3)
abline(x,y)
abline (x, y_hat)
abline (x, y_hat, col="red")
?abline
plot(x,y)
plot(x, y_hat)
abline(x, y_hat)
abline(y_hat, x, col="red")
abline(fit, col="green")
abline(ln(y~x))
abline(lm(y~x))
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
class(irisP)
irisP <- as.data.frame(irisP)
rownames(irisP)
irisP$Species
irisP
rownames(irisP)
irisP$Species <- rownames(irisP)
irisP$Species
irisP
Petal.Width
training
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
?qplot
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p
qplot(Petal.Width, Petal.Length, col=Species,data=training)
geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
table(pred,testing$Species)
setwd("C:/R/PracticalMachineLearning/")
trainFile <- "pml-training.csv"
train <- read.csv(trainFile, stringsAsFactors=FALSE)
str(train)
names(train)
nrow(train)
unique(train$user_name)
unique(train$X)
names(train)
unique(train$classe)
q<-subset(train, classe=="A")
q
names(train)
str(q)
table(train$user_name, classe)
table(train$user_name, train$classe)
q<-subset(train, classe=="A")
unique(q$classe)
unique(q$user_name)
train_pedro <- subset(q, q$user_name=="pedro")
unique(q$classe)
unique(q$user_name)
unique(train_pedro$classe)
unique(train_pedro$user_name)
nrow(train_pedro)
names(train)
train_pedro[1]
train_pedro[1,]
train_pedro[1:5,]
r<-train_pedro[1:5,]
r
sum(train$skewness_pitch_forearm != "")
z<-train$skewness_pitch_forearm
sum(z)
z
test <- read.csv(testFile, stringsAsFactors=FALSE)
testFile <- "pml_testing.csv"
test <- read.csv(testFile, stringsAsFactors=FALSE)
setwd("C:/R/PracticalMachineLearning/")
test <- read.csv(testFile, stringsAsFactors=FALSE)
getwd(0)
getwd()
dir()
testFile <- "pml-testing.csv"
test <- read.csv(testFile, stringsAsFactors=FALSE)
sum(test$skewness_pitch_forearm != "")
str(test)
train_names <- names(train)
test_names<-names(test)
train_names in test_names
train_names %in% test_names
length(test_names)
length(train_names)
test_names
train_names
sum(train_names %in% test_names)
length(train_names)
n = "accel_forearm_z"
train[,n]
is.na(train[,n])
?read.csv
n2 = "kurtosis_roll_belt"
is.na(train[,n2])
train[,n2]
test[,n2]
test <- read.csv(testFile, stringsAsFactors=FALSE, na.strings="NA")     # explicitly set NAs
train <- read.csv(trainFile, stringsAsFactors=FALSE, na.strings="NA")   # explicitly set NAs
sum(test[,n2])
sum(!is.na(test[,n2]))
use_name <- as.text()
for n in test_names{
if (sum(!is.na(test[,n])) > 0){
use_name <- c(use_name, n)
}
}
use_name <- as.character()
for n in test_names{
if (sum(!is.na(test[,n])) > 0){
use_name <- c(use_name, n)
}
}
use_name <- as.character()
for (n in test_names){
if (sum(!is.na(test[,n])) > 0){
use_name <- c(use_name, n)
}
}
use_name
test[,"pitch_arm"]
test[,"X"]
train[,"new_window"]
rm(list=ls())
setwd("C:/R/PracticalMachineLearning/")
#
# read in the data
trainFile <- "pml-training.csv"
testFile <- "pml-testing.csv"
train <- read.csv(trainFile, stringsAsFactors=FALSE, na.strings="NA")   # explicitly set NAs
realtest <- read.csv(testFile, stringsAsFactors=FALSE, na.strings="NA")     # explicitly set NAs
# see what data is there
train_names <- names(train)
realtest_names <- names(realtest)
# what is the difference between the two data sets?
length(train_names)
length(realtest_names)
sum(train_names %in% realtest_names)
# the difference between the variables in the two data sets is the last variable "classe" in train
# and "problem_id" in test
# What data is there for the test data?
use_name <- as.character()
for (n in realtest_names){
if (sum(!is.na(test[,n])) > 0){
use_name <- c(use_name, n)
}
}
# What data is there for the test data?
use_name <- as.character()
for (n in realtest_names){
if (sum(!is.na(realtest[,n])) > 0){
use_name <- c(use_name, n)
}
}
z<-use_name
class(use_name)
use_name
use_name[-"use_name"]
exclude_names <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window",
"num_window", "problem_id")
# What data is there for the test data?
use_name <- as.character()
for (n in realtest_names){
if (sum(!is.na(realtest[,n])) > 0){
if (! n %in% exclude_names){
use_name <- c(use_name, n)
}
}
}
use_name
z<-train[,use_name]
names(z)
use_name <- c("classe", use_name)
use_name
newtrain <- train[, use_name]
names(newtrain)
?createDataPartition
inTrain <- createDataPartition(y=classe, p=0.7, list=FALSE)
inTrain <- createDataPartition(y=newtrain$classe, p=0.7, list=FALSE)
sum(inTrain)
inTrain
inTrain2 <- createDataPartition(y=newtrain$classe, p=0.7, list=FALSE)
all.equal(inTrain, inTrain2)
set.seed(123)
inTrain2 <- createDataPartition(y=newtrain$classe, p=0.7, list=FALSE)
set.seed(123)
inTrain <- createDataPartition(y=newtrain$classe, p=0.7, list=FALSE)
all.equal(inTrain, inTrain2)
newtrain_train <- newtrain[inTrain]
newtrain_test <- newtrain[-inTrain]
nrow(newtrain_train)
class(newtrain_train)
newtrain_train
newtrain
str(newtrain)
str(newtrain_train)
newtrain_train <- newtrain[inTrain, ]
newtrain_test <- newtrain[-inTrain, ]
str(newtrain_train)
nrow(newtrain_train)
nrow(newtrain_test)
nrow(newtrain)
nrow(newtrain_train) + nrow(newtrain_test)
w_train <- train(classe ~ ., data=newtrain_train, method="glm")
warnings()
w_train <- train(classe ~ ., data=newtrain_train, method="rt")
w_train <- train(classe ~ ., data=newtrain_train, method="rf")
str(newtrain_train)
warnings()
t <- data.frame()
names <- names(newtrain_train)
for n (in names){
t <- rbind(t, data.frame(name=n, count=length(unique(newtrain_train[,n]))))
}
t <- data.frame()
names <- names(newtrain_train)
for (n in names){
t <- rbind(t, data.frame(name=n, count=length(unique(newtrain_train[,n]))))
}
t
library(caret)
w_train <- train(classe ~ ., data=newtrain_train, method="rf")
names(newtrain_train)
rm(list=ls())
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
library(caret)
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit
training
str(training)
training2 <- data.frame(lapply(training, as.character), stringsAsFactors=FALSE)
training2
str(training2)
?lapply
training2 <- training
head(training2)
str(training2)
training2$Species <- as.character(training2$Species)
head(training2)
str(training2)
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
rm(list=ls())
setwd("C:/R/PracticalMachineLearning/")
#
# read in the data
trainFile <- "pml-training.csv"
testFile <- "pml-testing.csv"
train <- read.csv(trainFile, stringsAsFactors=FALSE, na.strings="NA")   # explicitly set NAs
realtest <- read.csv(testFile, stringsAsFactors=FALSE, na.strings="NA")     # explicitly set NAs
# see what data is there
train_names <- names(train)
realtest_names <- names(realtest)
# what is the difference between the two data sets?
length(train_names)
length(realtest_names)
sum(train_names %in% realtest_names)
# the difference between the variables in the two data sets is the last variable "classe" in train
# and "problem_id" in test
exclude_names <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window",
"num_window", "problem_id")
# What data is there for the test data?
use_name <- as.character()
for (n in realtest_names){
if (sum(!is.na(realtest[,n])) > 0){
if (! n %in% exclude_names){
use_name <- c(use_name, n)
}
}
}
# add back the prediction
use_name <- c("classe", use_name)
# get the relevant columns
newtrain <- train[, use_name]
# split into training and test data sets
set.seed(123)
inTrain <- createDataPartition(y=newtrain$classe, p=0.7, list=FALSE)
newtrain_train <- newtrain[inTrain, ]
newtrain_test <- newtrain[-inTrain, ]
str(newtrain_train)
?train
train(newtrain[,-"classe"], "classe")
train(newtrain_train$classe ~ .)
train(newtrain_train$classe ~ ., data=newtrain_train[,-1])
train(newtrain_train$classe, data=newtrain_train[,-1])
train(y=newtrain_train$classe, data=newtrain_train[,-1])
train(y=newtrain_train$classe, x=newtrain_train[,-1])
modFit <- train(classe ~ ., method="gbm",data=newtrain_train,verbose=FALSE)
modFit <- train(classe ~ ., method="gbm",data=newtrain_train,verbose=FALSE)
library(gbm)
modFit <- train(classe ~ ., method="gbm",data=newtrain_train,verbose=FALSE)
modFit <- train(classe ~ ., method="gbm",data=newtrain_train,verbose=FALSE)
modFit <- train(classe ~ ., method="gbm",data=newtrain_train)
modFit <- train(classe ~ .,data=newtrain_train,method="rf",prox=TRUE)
warnings()
str(newtrain_train)
newtrain_train$classe <- as.numeric(newtrain_train$classe)
str(newtrain_train)
?factor
?factor
newtrain_train <- newtrain_test
newtrain_train
head(newtrain_train)
str(newtrain_train)
newtrain_train$classe <- as.factor(newtrain_train$classe)
str(newtrain_train)
newtrain_train$classe <- as.character(newtrain_train$classe)
str(newtrain_train)
newtrain_train$classe <- as.factor(newtrain_train$classe)
train_a <- subset(newtrain_train, newtrain_train$classe == "A")
nrow(train_a)
unique(newtrain_train$classe)
nrow(newtrain_train)
newtrain_train$classe <- as.character(newtrain_train.classe)
names(newtrain_train)
newtrain_train$classe <- as.character(newtrain_train$classe)
head(str(newtrain_train))
train_a2 <- subset(newtrain_train, newtrain_train$classe == "A")
nrow(train_a2)
nrow(train_a)
head(str(newtrain_train),5)
head(str(train_a), rows=5)
train_b <- train_a
train_b$classe <- factor(train_b$classe)
str(train_b)
train(classe ~ ., data=train_b, method="ada")
